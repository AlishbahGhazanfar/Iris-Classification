{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8047c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc2ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "0\n",
      "0\n",
      "Train Accuracy:0.9803921568627451\n",
      "Test Accuracy:0.9777777777777777\n",
      "Accuracy:0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"iris.csv\")\n",
    "# data.head()\n",
    "# print(\"Columns Rows\")\n",
    "# print(\"____________\")\n",
    "# data.shape\n",
    "#drop duplicates except fist\n",
    "data = data.drop_duplicates(keep='first')\n",
    "#checking duplicate values\n",
    "data.duplicated().sum()\n",
    "data['species'].unique()\n",
    "encoder = LabelEncoder()\n",
    "data['species'] = encoder.fit_transform(data.species)\n",
    "x = data.drop('species',axis=1)\n",
    "y = data['species']\n",
    "data\n",
    "x_train , x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Apply feature selection using SelectKBest with chi-squared test\n",
    "k = 2  # Number of features to select\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_new = selector.fit_transform(x, y)\n",
    "\n",
    "# Get the selected feature indices\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_feature_names = [data.species[i] for i in selected_feature_indices]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"Selected Features:\")\n",
    "for feature in selected_feature_names:\n",
    "    print(feature)\n",
    "    \n",
    "    \n",
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "train_acc = model.score(x_train,y_train)\n",
    "test_acc = model.score(x_test,y_test)\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "\n",
    "print(f'Train Accuracy:{train_acc}')\n",
    "print(f'Test Accuracy:{test_acc}')\n",
    "print(f'Accuracy:{accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f2e3216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Species:\n",
      "setosa\n",
      "setosa\n",
      "versicolor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_data = [ [2.1, 3.0, 4.7, 1.0],[5.1, 3.5, 1.4, 0.2],  # example features of a new iris flower\n",
    "            [6.2, 2.9, 4.3, 1.3]]\n",
    "\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "predictions = model.predict(new_data_scaled)\n",
    "\n",
    "# Decode the predictions back to original species names\n",
    "predicted_species = encoder.inverse_transform(predictions)\n",
    "\n",
    "print(\"Predicted Species:\")\n",
    "for species in predicted_species:\n",
    "    print(species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6a51a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_regression_model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e40818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model\n",
    "model = joblib.load('logistic_regression_model.pkl')\n",
    "\n",
    "# Preprocess the new data\n",
    "new_data = np.array([[2.1, 5.6,6.2, 3.2], [5.1, 3.5, 1.4, 0.2], [6.2, 2.9, 4.3, 1.3]])\n",
    "new_data_scaled = scaler.transform(new_data)  # Assuming 'scaler' is the StandardScaler used for preprocessing\n",
    "\n",
    "# Make predictions on the new data\n",
    "predictions = model.predict(new_data_scaled)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89813e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'setosa' 'versicolor']\n"
     ]
    }
   ],
   "source": [
    "# predicted_class = encoder.inverse_transform([2])[0]\n",
    "# print(predicted_class)\n",
    "# Assuming 'encoder' is the LabelEncoder used for encoding the target variable\n",
    "predicted_class = encoder.inverse_transform(predictions)\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca0d673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "855\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.getsize('logistic_regression_model.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea9740",
   "metadata": {},
   "source": [
    "### FASTAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e138d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "from BankNotes import BankNote\n",
    "from fastapi import FastAPI\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c50e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = joblib.load('logistic_regression_model.pkl')\n",
    "# scaler = StandardScaler()  # Assuming 'scaler' is the StandardScaler used for preprocessing\n",
    "\n",
    "# Create a FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define a Pydantic model for the request body\n",
    "class IrisData(BaseModel):\n",
    "    sepal_length: float\n",
    "    sepal_width: float\n",
    "    petal_length: float\n",
    "    petal_width: float\n",
    "\n",
    "# Define a route for the prediction endpoint\n",
    "@app.post(\"/predict\")\n",
    "def predict_species(data: IrisData):\n",
    "    try:\n",
    "        # Prepare the data for prediction\n",
    "        new_data = np.array([[data.sepal_length, data.sepal_width, data.petal_length, data.petal_width]])\n",
    "        new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "        # Make the prediction\n",
    "        prediction = model.predict(new_data_scaled)\n",
    "\n",
    "        # Return the predicted class\n",
    "        return {\"predicted_species\": int(prediction[0])}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc360e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [11316]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:56979 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     ::1:56980 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56980 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:56981 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:57000 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     ::1:57001 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57001 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:57003 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "nest_asyncio.apply()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1256cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
